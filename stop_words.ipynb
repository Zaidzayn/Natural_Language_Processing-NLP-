{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Dear Students, Dear Friends,\n",
    "I greet all of you with great joy and hope. Today, I stand before you not as a former President, but as someone who believes deeply in the power of dreams and the strength of our youth.\n",
    "Each one of you has within you the spark of greatness. You are not just students; you are the architects of tomorrow's India. When I look at your bright faces, I see scientists who will unlock the mysteries of the universe, engineers who will build bridges to the future, doctors who will heal humanity, and leaders who will guide our nation to new heights.\n",
    "Remember, dreams are not what you see in your sleep; dreams are the things that don't let you sleep. Let your dreams be so big that they ignite your passion and drive you to achieve the impossible. I have seen a small boy from Rameswaram, who sold newspapers to support his family, reach the highest office in the land. If that is possible, then everything is possible for you.\n",
    "Three powers will guide you in your journey:\n",
    "\n",
    "The power of knowledge - Keep learning, keep questioning, keep exploring\n",
    "The power of perseverance - Never give up, even when the path seems impossible\n",
    "The power of integrity - Let honesty and righteousness be your guiding stars\n",
    "\n",
    "India needs you. Our nation is calling for young minds who can transform challenges into opportunities, who can turn our villages into centers of prosperity, and who can make India a developed nation where every citizen lives with dignity and hope.\n",
    "Work with dedication. Study with passion. Serve with humility. And remember - you are unique, you are special, and you have the power to change the world.\n",
    "May you all become the ignited minds that will light up our beautiful India!\n",
    "Thank you, and may God bless you all.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.downloader import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "PorterStemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Apply stopwords and filter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[PorterStemmer.stem(word) for word in words if word not in stopwords.words('english') and word.isalpha()] \n",
    "    sentences[i]=' '.join(words)## Conerting all words into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear student dear friend greet great joy hope',\n",
       " 'today stand former presid someon believ deepli power dream strength youth',\n",
       " 'one within spark great',\n",
       " 'student architect tomorrow india',\n",
       " 'look bright face see scientist unlock mysteri univ engin build bridg futur doctor heal human leader guid nation new height',\n",
       " 'rememb dream see sleep dream thing let sleep',\n",
       " 'let dream big ignit passion drive achiev imposs',\n",
       " 'seen small boy rameswaram sold newspap support famili reach highest offic land',\n",
       " 'possibl everyth possibl',\n",
       " 'three power guid journey power knowledg keep learn keep question keep explor power persev never give even path seem imposs power integr let honesti righteou guid star india need',\n",
       " 'nation call young mind transform challeng opportun turn villag center prosper make india develop nation everi citizen live digniti hope',\n",
       " 'work dedic',\n",
       " 'studi passion',\n",
       " 'serv humil',\n",
       " 'rememb uniqu special power chang world',\n",
       " 'may becom ignit mind light beauti india',\n",
       " 'thank may god bless']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer = SnowballStemmer('english')\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[SnowballStemmer.stem(word) for word in words if word not in stopwords.words('english') and word.isalpha()] \n",
    "    sentences[i]=' '.join(words)## Conerting all words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear student dear friend greet great joy hope',\n",
       " 'today stand former presid someon believ deepli power dream strength youth',\n",
       " 'one within spark great',\n",
       " 'student architect tomorrow india',\n",
       " 'look bright face see scientist unlock mysteri univ engin build bridg futur doctor heal human leader guid nation new height',\n",
       " 'rememb dream see sleep dream thing let sleep',\n",
       " 'let dream big ignit passion drive achiev imposs',\n",
       " 'seen small boy rameswaram sold newspap support famili reach highest offic land',\n",
       " 'possibl everyth possibl',\n",
       " 'three power guid journey power knowledg keep learn keep question keep explor power persev never give even path seem imposs power integr let honesti righteou guid star india need',\n",
       " 'nation call young mind transform challeng opportun turn villag center prosper make india develop nation everi citizen live digniti hope',\n",
       " 'work dedic',\n",
       " 'studi passion',\n",
       " 'serv humil',\n",
       " 'rememb uniqu special power chang world',\n",
       " 'may becom ignit mind light beauti india',\n",
       " 'thank may god bless']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "WordNetLemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[WordNetLemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english') and word.isalpha()] \n",
    "    sentences[i]=' '.join(words)## Conerting all words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear student dear friend greet great joy hope',\n",
       " 'today stand former presid someon believ deepli power dream strength youth',\n",
       " 'one within spark great',\n",
       " 'student architect tomorrow india',\n",
       " 'look bright face see scientist unlock mysteri univ engin build bridg futur doctor heal human leader guid nation new height',\n",
       " 'rememb dream see sleep dream thing let sleep',\n",
       " 'let dream big ignit passion drive achiev imposs',\n",
       " 'seen small boy rameswaram sold newspap support famili reach highest offic land',\n",
       " 'possibl everyth possibl',\n",
       " 'three power guid journey power knowledg keep learn keep question keep explor power persev never give even path seem imposs power integr let honesti righteou guid star india need',\n",
       " 'nation call young mind transform challeng opportun turn villag center prosper make india develop nation everi citizen live digniti hope',\n",
       " 'work dedic',\n",
       " 'studi passion',\n",
       " 'serv humil',\n",
       " 'rememb uniqu special power chang world',\n",
       " 'may becom ignit mind light beauti india',\n",
       " 'thank may god bless']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
